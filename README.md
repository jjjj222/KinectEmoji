Human emotion detection has been a wide area of research for a very long time. Different ways to detect humans have been researched. Detecting expressions from static images has been done using different classifiers such as Neural Networks or SVMs. Detecting expression from video poses more challenges such as evaluating each frame in real time and figuring out the emotion. Emotion detection has applications in various domains. As humans interact more and more with computers and devices, taking emotions into account will play a vital role in deciding how humans interact with computers. In this work we present a method to detect human facial expressions using the Microsoft Kinect 2. We utilize the FaceHD and FaceBasics data from the kinect to classify expressions and display an emoji accordingly. We present our results in classifying 8 different emojis. We use a gesture based approach to classify the emotions. Each gesture is then mapped to a particular emoji as shown in the results. We calculate threshold values for our conditional statements These threshold values are calculated empirically and we show their performance with our experiments. We also show that just with a few samples from the face data we are able to get good enough features to classify different emojis.


- [Report](https://github.com/jjjj222/KinectEmoji/blob/master/doc/proceedings.pdf)
